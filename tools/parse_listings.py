#!/usr/bin/env python3
"""
Tool: VivaReal Parser
Extrai dados estruturados de p√°ginas HTML crawleadas do VivaReal.
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Optional
from bs4 import BeautifulSoup

class VivaRealParser:
    def __init__(self, input_dir: str = "data/raw", output_dir: str = "data/processed"):
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def extract_price(self, text: str) -> Optional[float]:
        """Extrai valor em reais de texto."""
        if not text:
            return None

        # Remove tudo exceto d√≠gitos
        digits = re.sub(r'[^\d]', '', text)

        try:
            return float(digits) if digits else None
        except ValueError:
            return None

    def extract_area(self, text: str) -> Optional[float]:
        """Extrai √°rea em m¬≤ de texto."""
        if not text:
            return None

        # Procura padr√£o: n√∫mero + m¬≤ ou m2
        match = re.search(r'(\d+(?:[.,]\d+)?)\s*m[¬≤2]', text.lower())
        if match:
            area_str = match.group(1).replace(',', '.')
            try:
                return float(area_str)
            except ValueError:
                return None

        return None

    def extract_listing(self, card_element) -> Optional[Dict]:
        """
        Extrai dados de um card de an√∫ncio.

        Estrutura t√≠pica do VivaReal:
        - Link: <a href="..."> no card principal
        - Pre√ßo: class contendo "price" ou texto com R$
        - √Årea: texto contendo "m¬≤"
        """
        try:
            # Extrair link
            link_elem = card_element.find('a', href=True)
            if not link_elem:
                return None

            link = link_elem.get('href', '')
            if link.startswith('/'):
                link = f"https://www.vivareal.com.br{link}"

            # Extrair pre√ßo
            price_text = None
            price_selectors = [
                {'class_': re.compile(r'price', re.I)},
                {'attrs': {'data-type': 'price'}},
                {'string': re.compile(r'R\$', re.I)},
            ]

            for selector in price_selectors:
                price_elem = card_element.find(['p', 'span', 'div'], **selector)
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    break

            price = self.extract_price(price_text) if price_text else None

            # Extrair √°rea
            area_text = card_element.get_text()
            area = self.extract_area(area_text)

            # Validar dados m√≠nimos
            if not link or not price or not area:
                return None

            return {
                "link": link,
                "price": price,
                "area": area,
                "price_per_sqm": round(price / area, 2) if area > 0 else None
            }

        except Exception as e:
            print(f"‚ö†Ô∏è  Erro ao processar card: {e}")
            return None

    def parse_page(self, html_path: Path) -> List[Dict]:
        """Parseia uma p√°gina HTML e extrai todos os an√∫ncios."""
        print(f"üìÑ Parsing: {html_path.name}")

        with open(html_path, 'r', encoding='utf-8') as f:
            html_content = f.read()

        soup = BeautifulSoup(html_content, 'lxml')

        # Tentar identificar cards de an√∫ncios
        # VivaReal geralmente usa classes como "property-card", "result-card", etc.
        card_selectors = [
            {'class_': re.compile(r'property.*card', re.I)},
            {'class_': re.compile(r'result.*card', re.I)},
            {'class_': re.compile(r'listing.*card', re.I)},
            {'attrs': {'data-type': 'property'}},
        ]

        cards = []
        for selector in card_selectors:
            found = soup.find_all(['div', 'article', 'li'], **selector)
            if found:
                cards = found
                print(f"   ‚úì Encontrados {len(cards)} cards com seletor: {selector}")
                break

        # Se n√£o encontrou cards espec√≠ficos, tentar abordagem alternativa
        if not cards:
            # Procurar por links que contenham "/imovel/"
            links = soup.find_all('a', href=re.compile(r'/imovel/'))
            # Pegar parent elements como "cards"
            cards = list(set([link.find_parent(['div', 'article', 'li']) for link in links if link.find_parent(['div', 'article', 'li'])]))
            print(f"   ‚úì Encontrados {len(cards)} cards via links")

        # Extrair dados de cada card
        listings = []
        for card in cards:
            listing = self.extract_listing(card)
            if listing:
                listings.append(listing)

        print(f"   ‚úÖ Extra√≠dos {len(listings)} an√∫ncios v√°lidos")
        return listings

    def parse_all(self, min_area: float = 40, max_area: float = 45) -> List[Dict]:
        """
        Parseia todas as p√°ginas HTML no diret√≥rio de entrada.

        Args:
            min_area: Filtro de √°rea m√≠nima
            max_area: Filtro de √°rea m√°xima

        Returns:
            Lista de an√∫ncios filtrados e validados
        """
        print(f"\nüîç Iniciando parsing de arquivos HTML")
        print(f"   Input dir: {self.input_dir}")
        print(f"   Filtro √°rea: {min_area}-{max_area} m¬≤\n")

        html_files = sorted(self.input_dir.glob("page_*.html"))

        if not html_files:
            print("‚ùå Nenhum arquivo HTML encontrado!")
            return []

        all_listings = []

        for html_file in html_files:
            page_listings = self.parse_page(html_file)
            all_listings.extend(page_listings)

        # Filtrar por √°rea
        filtered_listings = [
            listing for listing in all_listings
            if min_area <= listing['area'] <= max_area
        ]

        # Remover duplicatas (mesmo link)
        unique_listings = {}
        for listing in filtered_listings:
            unique_listings[listing['link']] = listing

        final_listings = list(unique_listings.values())

        print(f"\nüìä Resumo do Parsing:")
        print(f"   Total extra√≠do: {len(all_listings)}")
        print(f"   Ap√≥s filtro de √°rea: {len(filtered_listings)}")
        print(f"   Ap√≥s remo√ß√£o de duplicatas: {len(final_listings)}")

        # Salvar resultado
        output_path = self.output_dir / "listings.json"
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(final_listings, f, indent=2, ensure_ascii=False)

        print(f"\n‚úÖ Dados salvos: {output_path}")

        return final_listings


def main():
    """CLI para execu√ß√£o standalone."""
    import argparse

    parser = argparse.ArgumentParser(description="Parse VivaReal HTML files")
    parser.add_argument("--input", default="data/raw", help="Diret√≥rio de entrada (HTML)")
    parser.add_argument("--output", default="data/processed", help="Diret√≥rio de sa√≠da (JSON)")
    parser.add_argument("--min-area", type=float, default=40, help="√Årea m√≠nima (m¬≤)")
    parser.add_argument("--max-area", type=float, default=45, help="√Årea m√°xima (m¬≤)")

    args = parser.parse_args()

    parser = VivaRealParser(input_dir=args.input, output_dir=args.output)
    listings = parser.parse_all(min_area=args.min_area, max_area=args.max_area)

    if listings:
        print(f"\nüéâ Sucesso! {len(listings)} an√∫ncios prontos para an√°lise")
    else:
        print("\n‚ö†Ô∏è  Nenhum an√∫ncio v√°lido encontrado")


if __name__ == "__main__":
    main()
